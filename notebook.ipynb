{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tdx.components import CsvExampleGen, StatisticGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.orchestration.experimental.interacrive.interactive_context import InteractiveContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"sarcasm-pipeline\"\n",
    "SCHAME_PIPELINE_NAME = \"sarcasm-tfdv-schema\"\n",
    "\n",
    "# Directory untuk menyimpan artifact yang akan dihasilkan\n",
    "PIPELINE_ROOT = os.path.join('pipeline', PIPELINE_NAME)\n",
    "\n",
    "# path to SQLite DB file to use as an MLMD storage\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "\n",
    "# Output directory where created models from the pipeline will be exported\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "# from abs1 import logging\n",
    "# logging.set_varbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat Tahapan Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = example_gen_pb2.Output(\n",
    "    split_config = example_gen_pb2.SplitConfig(splits=[\n",
    "        # membuat rasio split 8:2\n",
    "        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_bucket=8),\n",
    "        example_gen_pb2.SplitConfig.Split(name=\"valid\", hash_bucket=2)\n",
    "    ])\n",
    ")\n",
    "\n",
    "example_gen = CsvExampleGen(input_base = DATA_ROOT, output_config=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jika mau melihat komponen ExampleGen secara interaktif\n",
    "interactive_context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat Tahapan Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.membuat summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = StatisticGen(\n",
    "    example=example_gen.outputs[\"examples\"]\n",
    ")\n",
    "\n",
    "interactive_context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.menampilkan summary statistics yang sudah dibuat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.show(statistics_gen.output[\"statistics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.menampilkan data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "interactive_context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan data schema yang sudah dibuat\n",
    "interactive_context.show(schema_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.mengidentifikasi anomali pada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.output['statistics'],\n",
    "    schema=schema_gen.outputs['schema']\n",
    ")\n",
    "interactive_context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkan hasil validasi komponen\n",
    "interactive_context.show(example_validator.output['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat Tahapan Data Preprocessing\n",
    "menggunakan TFT dan komponen Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisikan modul dulu\n",
    "TRANSFORM_MODULE_FILE = \"sarcasm_transform.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic coammand untuk membuat modul, ini khusus hanya di jupyter\n",
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\n",
    "import tensorflow as td\n",
    "LABEL_KEY = \"is_sarcastic\"\n",
    "FEATURE_KEY = \"headline\"\n",
    "\n",
    "# digunakan untuk mengubah nama fitur yang sudah di transform\n",
    "def transformed_name(key):\n",
    "    \"\"\"Remaining transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "# menggunakan preprocessing sedergana. transform \"headline\" ke dalam bentuk lowercase dan \"is_sarcasic\" ke dalam bentuk integer\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features\n",
    "\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw featyres.\n",
    "        \n",
    "    Return:\n",
    "        outputs: map form feature keys to transformed features\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    outpus[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
    "    \n",
    "    ouputs[transformed_name(LABEL_KEY)] = TF.cast(inputs[LABEL_KEY], tf.int64)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mendefinisikan transform\n",
    "transform = Transform(\n",
    "    example_gen = example_gen.outputs['examples'],\n",
    "    schema = schema_gen.outputs['schema'],\n",
    "    module_file = os.path.abspath(TRANSFORM_MODULE_FILE)\n",
    ")\n",
    "interactive_context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengunaan ExampleGen di GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import ImportExampleGen\n",
    "example_gen = ImportExampleGen(input_base = \"tfrecord_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cloud storage Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "\n",
    "example_gen = CsvExampleGen(input_base = \"gs://bucket_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Google Cloud BigQuery: platform DWH dari GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.components import BigQueryExampleGen\n",
    "\n",
    "# Mengatur Google Cloud credential\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path.json\"\n",
    "\n",
    "query = \"SELECT * FROM <project_id>.<database>.<table_name>\"\n",
    "\n",
    "example_gen = BigQueryExampleGen(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melakukan data Splitting menggunakan Subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "└── data\n",
    "├── train\n",
    "│ └─ iris-training.csv\n",
    "├── eval\n",
    "│ └─ iris-eval.csv\n",
    "└── test\n",
    "└─ iris-test.csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = example_gen_pb2.Input(splits=[\n",
    "    example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "    example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),\n",
    "    example_gen_pb2.Input.Split(name='test', pattern='test/*')\n",
    "])\n",
    "\n",
    "example_gen = CsvExampleGen(input_base=\"data\", input_config=input)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "└── data\n",
    "├── export-0\n",
    "│ ├── train\n",
    "│ │ └─ 20k-iris-training.csv\n",
    "│ └── eval\n",
    "│ └─ 2k-iris-eval.csv\n",
    "├── export-1\n",
    "│ ├── train\n",
    "│ │ └─ 24k-iris-training.csv\n",
    "│ └── eval\n",
    "│ └─ 3k-iris-eval.csv\n",
    "└── export-2\n",
    "├── train\n",
    "│ └─ 26k-iris-training.csv\n",
    "└── eval\n",
    "  └─ 4k-iris-eval.csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = example_gen_pb2.Input(splits=[\n",
    "    example_gen_pb2.Input.Split(name='train', pattern='export-{SPAM}/train/*'),\n",
    "    example_gen_pb2.Input.Split(name='eval', pattern='export-{SPAN}/eval/*')\n",
    "])\n",
    "\n",
    "example_gen = CsvExampleGen(input_base=\"data\", input_config=input)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengolah text data\n",
    "- tft.compute_and_apply_vocabulary(): fungsi ini membuat sebuah vocabulary yang menghubungkan sebuah string dengan suatu integer.\n",
    "\n",
    "- tft.ngrams(): fungsi ini menerima input berupa berupa token dengan tipe data SparseTensor. Selanjutnya fungsi ini akan menghasilkan sebuah SparseTensor yang berisi n-grams.\n",
    "\n",
    "- tft.bag_of_words(): fungsi ini akan menghasilkan sebuah vektor bag-of-words berdasarkan n-grams.\n",
    "\n",
    "- tft.tfidf(): fungsi ini akan melakukan proses TFIDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "  s = inputs['s']\n",
    " \n",
    "  s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    " \n",
    "  return {\n",
    "      's_integerized': s_integerized\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengolah image data\n",
    "- tf.image menyediakan beberapa fungsi yang dapat digunakan untuk memanipulasi image seperti resize, convert color, image transformation\n",
    "\n",
    "- tf.io digunakan untuk melakukan dekode gambar menjadi bentuk tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(new_image):\n",
    "    raw_image = tf.reshape(raw_image, [-1])\n",
    "    \n",
    "    image_rgb = tf.io.decode_jpeg(raw_image, channels=3)\n",
    "    image_gray = tf.image.rgb_to_greyscale(image_rgb)\n",
    "    \n",
    "    image = tf.image_convert_image_dtype(image_gray, tf.float32)\n",
    "    resize_image = tf.image.resize_with_pad(\n",
    "        image,\n",
    "        target_height = 150,\n",
    "        target_width = 150\n",
    "    )\n",
    "    \n",
    "    return tf.reshape(resize_image, [-1, 150, 150, 1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
